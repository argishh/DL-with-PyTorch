{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating **custom dataset** for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example implementation framework\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Defining dataset processing class\n",
    "class WaterDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__()\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.data = df.to_numpy()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx, :-1]\n",
    "        label = self.data[idx, -1]\n",
    "        return features, label\n",
    "\n",
    "\n",
    "# creating class object and loading the data in the object.\n",
    "dataset_train = WaterDataset('water_train.csv')\n",
    "\n",
    "# loading the data from the class object using dataloader\n",
    "dataloader_train = DataLoader(dataset_train,\n",
    "                              batch_size = 2,\n",
    "                              shuffle = True)\n",
    "\n",
    "features, labels = next(iter(dataloader_train))\n",
    "print(f'Features {features}\\nLabels: {labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sequential** VS **Class-based** model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Model Definition\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(9, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-based model definition\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16,8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Using Binary Cross Entropy Loss function (BCELoss) which is commonly used for binary classification\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    for features, labels in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(features)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "acc = Accuracy(task='binary')\n",
    "\n",
    "# putting model in evaluation mode\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in dataloader_train:\n",
    "        outputs = net(features)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        acc(preds, labels.view(-1, 1))\n",
    "\n",
    "accuracy = acc.compute()\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing and Exploding Gradients\n",
    "\n",
    "Gradients that get smaller and smaller during backward pass are called Vanishing gradients. here, the earlier layers get small parameter updates and the model doesn't learn.\n",
    "\n",
    "Gradients that get bigger and bigger are called exploding gradients. here, the paarameter updates are too large and the training diverges.\n",
    "\n",
    "To address this problem, we need :-\n",
    "- Proper weight Initialization\n",
    "- Good Activations\n",
    "- Batch Normalization\n",
    "\n",
    "1. **Proper weight Initialization**\n",
    "    - ensures that the variance of layer inputs = vairnace of layer outputs\n",
    "    - variance of gradients are the same before and after a layer.\n",
    "    - We can use He/Kaiming weight Initialization for ReLU and similar actv. functions.\n",
    "    - e.g. :-\n",
    "        ```py\n",
    "        import torch.nn.init as init\n",
    "\n",
    "        init.kaiming_uniform_(layer.weight)\n",
    "        print(layer.weight)\n",
    "        ```\n",
    "    - for actv. using classes :-\n",
    "        ```py\n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity='sigmoid')\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# He/Kaiming Weight initialization implementation-\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "    \n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity='sigmoid')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Activation Functions**\n",
    "   - ReLU is often used as default activation \n",
    "   - It can be summoned using `nn.functional.relu()`\n",
    "   - Drawback - it suffers from dying neurons problem (zero for negative inputs)\n",
    "   - ELU (Exponential linear unit) is an alternative that avoids dying neurons problem. \n",
    "   - It's non-zero gradients for negative values helps against dying neurons.\n",
    "   - It's average output is near zero, so it helps against vanishing gradients.\n",
    "   - It can be summoned using `nn.functional.elu()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Batch Normalization** - After adding a layer, follow the steps -\n",
    "\n",
    "   1.  Noramalize the layer's outputs by \n",
    "       - subtracting the mean\n",
    "       - dividing by the standard deviation\n",
    "   \n",
    "   2. Scale and shift normalized outputs using learnned parameters\n",
    "      -   Model Learns optimal inputs distribution for each layer and helps in\n",
    "          -   faster loss decrease,\n",
    "          -   against unstable gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization example -\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.elu(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
