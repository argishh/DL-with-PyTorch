{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation Metrics for Text Classification**\n",
    "\n",
    "Example Scenario - \\\n",
    "A model trained to predict sentiment of book reviews tells that the best seller has mostly negative reviews.\n",
    "Should this judgement be accepted?\n",
    "\n",
    "Let us first generate predictions before evaluating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "rnn_model = RNNModel(input_size, hidden_siee, num_layers, num_classes)\n",
    "# ...\n",
    "\n",
    "# Model Training\n",
    "for epoch in range(10):\n",
    "    outputs = rnn_model(X_train)\n",
    "    # ...\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.itme()}')\n",
    "\n",
    "outputs = rnn_model(X_test)\n",
    "_, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `predicted` sentiments for evaluation.\n",
    "\n",
    "The most straightforward metric is accuracy. We can calculate the ratio of correct predictions to the total predictions to obtain accuracty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 66.6667%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "actual = torch.tensor([0, 1, 1, 0, 1, 0])\n",
    "predicted = torch.tensor([0, 0, 1, 0, 1, 1])\n",
    "\n",
    "accuracy = Accuracy(task='binary', num_classes=2)\n",
    "accuracy_score = accuracy(predicted, actual)\n",
    "\n",
    "print(f'Accuracy {accuracy_score*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factors beyond accuracy -\n",
    "- If the model is trained on 10,000 reviews, out of which 9,800 are positive, \\\n",
    "  then the model will always predict positive with 98% accuracy.\n",
    "\n",
    "#### **Other Metrics to consider**\n",
    "\n",
    "- Precision: shows confidence in labelling a review as negative\n",
    "- Recall: shows how well the model spots negative reviews\n",
    "- F1 Score: finds a balance between precision and recall\n",
    "\n",
    "Focusing on accuracy alone can lead us to miss significant feedback on model's performace. \\\n",
    "Thus, we should consider adding more evaluation metrics to better understand the model's decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Precision and Recall**\n",
    "\n",
    "- Precision :\n",
    "$$\n",
    "\\Large\\frac{Correctly \\; Predicted \\; Positive \\; Observations}{Total \\; Predicted \\; Positives}\n",
    "$$\n",
    "\n",
    "- Recall:\n",
    "\n",
    "$$\n",
    "\\Large\\frac{Correctly \\; Predicted \\; Positive \\; Observations}{All \\; Observations \\; in \\; Positive \\; Class}\n",
    "$$\n",
    "\n",
    "<!-- $ \\Large\\frac{Correctly \\; Predicted \\; Positive \\; Observations}{All \\; Observations \\; in \\; Positive \\; Class} $ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6667\n",
      "Recall: 0.6667\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Precision, Recall\n",
    "\n",
    "precision = Precision(task='binary', num_classes=2)\n",
    "recall = Recall(task='binary', num_classes=2)\n",
    "\n",
    "prec = precision(predicted, actual)\n",
    "rec = recall(predicted, actual)\n",
    "\n",
    "print(f'Precision: {prec:.4f}')\n",
    "print(f'Recall: {rec:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **F1 Score**\n",
    "\n",
    "- Harmonizes precision and recall\n",
    "- Better measure for imbalanced classes\n",
    "\n",
    "F1 Score scale -\n",
    "- F1 Score of 1 = perfect precision and recall\n",
    "- F1 score of 0 = worst performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import F1Score\n",
    "\n",
    "f1 = F1Score(task='binary', num_classes=2)\n",
    "f1_score = f1(predicted, actual)\n",
    "\n",
    "print(f'F1 Score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
